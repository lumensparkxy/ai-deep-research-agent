{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ccfbc2-2095-42bd-b799-174b144ef773",
   "metadata": {},
   "source": [
    "# Deep Research Agent - ProjectSage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5153d6-d1a4-4ee4-9949-ab6722357561",
   "metadata": {},
   "source": [
    "## 1 Project Snapshot\n",
    "\n",
    "**One-sentence elevator pitch:** A comprehensive AI-powered decision support system that provides universal decision-making assistance through a sophisticated 6-stage iterative research process powered by Google Gemini 2.5 Pro with dynamic personalization.\n",
    "\n",
    "**Tech stack & major third-party deps:**\n",
    "- Python 3.9+ with Google Gemini AI API integration\n",
    "- Core deps: `google-genai`, `python-dotenv`, `pyyaml` \n",
    "- Dev/test deps: `pytest`, `pytest-cov`, `black`\n",
    "- Architecture: Modular OOP with async AI integration patterns\n",
    "\n",
    "**Directory & module map:**\n",
    "```\n",
    "702_gemini_deep_research/\n",
    "├── main.py                         # CLI entry point & signal handling\n",
    "├── config/\n",
    "│   ├── settings.py                 # Configuration management class\n",
    "│   └── settings.yaml               # YAML config with defaults\n",
    "├── core/                           # Business logic modules\n",
    "│   ├── research_engine.py          # 6-stage research orchestrator\n",
    "│   ├── conversation.py             # User interaction handler\n",
    "│   ├── report_generator.py         # Markdown report creation\n",
    "│   ├── dynamic_personalization.py  # AI conversation engine\n",
    "│   ├── conversation_mode_intelligence.py # Adaptive conversation modes\n",
    "│   ├── ai_question_generator.py    # Gemini question generation\n",
    "│   ├── context_analyzer.py         # Response analysis\n",
    "│   └── conversation_state.py       # State management\n",
    "├── utils/\n",
    "│   ├── session_manager.py          # JSON session persistence\n",
    "│   └── validators.py               # Input sanitization\n",
    "├── tests/                          # Comprehensive test suite (92 tests)\n",
    "└── data/\n",
    "    ├── sessions/                   # Persisted research sessions\n",
    "    └── reports/                    # Generated markdown reports\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9f896-71c8-40a7-9e3e-dec817c75a05",
   "metadata": {},
   "source": [
    "## 2 Architectural Overview\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[main.py CLI] --> B[ConversationHandler]\n",
    "    B --> C[DynamicPersonalizationEngine]\n",
    "    B --> D[ResearchEngine]\n",
    "    B --> E[ReportGenerator]\n",
    "    \n",
    "    C --> F[ConversationModeIntelligence]\n",
    "    C --> G[AIQuestionGenerator]\n",
    "    C --> H[ContextAnalyzer]\n",
    "    \n",
    "    D --> I[Gemini API Client]\n",
    "    G --> I\n",
    "    H --> I\n",
    "    \n",
    "    J[SessionManager] --> K[JSON Storage]\n",
    "    L[InputValidator] --> M[Security Sanitization]\n",
    "    \n",
    "    N[Settings] --> O[YAML Config]\n",
    "    N --> P[Environment Variables]\n",
    "    \n",
    "    B -.-> J\n",
    "    D -.-> J\n",
    "    E -.-> J\n",
    "    \n",
    "    style I fill:#e1f5fe\n",
    "    style K fill:#f3e5f5\n",
    "    style M fill:#ffebee\n",
    "```\n",
    "\n",
    "**Data-flow narrative:** User queries enter through CLI → conversation handler gathers personalization through AI-driven questioning → research engine conducts 6-stage iterative research using Gemini → results synthesized into professional markdown reports → all data persisted in JSON sessions.\n",
    "\n",
    "**Key design patterns:**\n",
    "- **Strategy Pattern**: Multiple conversation modes (Quick/Standard/Deep/Adaptive)\n",
    "- **Observer Pattern**: Real-time progress feedback during research stages\n",
    "- **Factory Pattern**: Report generation with configurable depth levels\n",
    "- **Command Pattern**: Modular research stage execution with fallback handling\n",
    "- **State Machine**: Conversation state management with AI-driven transitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cbb8e-ef9a-4ed2-a348-84ed1ac6f497",
   "metadata": {},
   "source": [
    "## 3 Execution Flow Diagrams\n",
    "\n",
    "### Main CLI Entry Point\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[python main.py] --> B{Parse Arguments}\n",
    "    B -->|--list-sessions| C[List Sessions & Exit]\n",
    "    B -->|--cleanup N| D[Cleanup Old Sessions & Exit]\n",
    "    B -->|Default| E[Setup Signal Handlers]\n",
    "    \n",
    "    E --> F[Initialize Settings]\n",
    "    F --> G[Setup Logging]\n",
    "    G --> H[Create ConversationHandler]\n",
    "    H --> I[Start Interactive Session]\n",
    "    \n",
    "    I --> J[Print Welcome Banner]\n",
    "    J --> K[Get Research Query]\n",
    "    K --> L{Personalization?}\n",
    "    \n",
    "    L -->|Yes| M[Dynamic AI Personalization]\n",
    "    L -->|No| N[Basic Context]\n",
    "    \n",
    "    M --> O[Create Session]\n",
    "    N --> O\n",
    "    O --> P[ResearchEngine.conduct_research]\n",
    "    P --> Q[ReportGenerator.generate_report]\n",
    "    Q --> R[Display Results & Exit]\n",
    "    \n",
    "    style P fill:#e3f2fd\n",
    "    style M fill:#e8f5e8\n",
    "```\n",
    "\n",
    "### Research Engine Cross-Module Sequence\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant C as ConversationHandler\n",
    "    participant RE as ResearchEngine\n",
    "    participant G as Gemini API\n",
    "    participant SM as SessionManager\n",
    "    participant RG as ReportGenerator\n",
    "    \n",
    "    C->>SM: create_session(query, context)\n",
    "    SM-->>C: session_data\n",
    "    \n",
    "    C->>RE: conduct_research(query, context, session_id)\n",
    "    \n",
    "    loop 6 Research Stages\n",
    "        RE->>G: generate_content(stage_prompt)\n",
    "        G-->>RE: research_findings\n",
    "        RE->>SM: update_session_stage(session_id, stage_data)\n",
    "        RE->>C: display_stage_progress(stage_num, stage_name)\n",
    "    end\n",
    "    \n",
    "    RE->>SM: update_session_conclusions(session_id, conclusions, confidence)\n",
    "    RE-->>C: final_results\n",
    "    \n",
    "    C->>RG: generate_report(session_data, results, depth)\n",
    "    RG-->>C: report_path\n",
    "    \n",
    "    C->>SM: update_session_report_path(session_id, report_path)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c34aa-35dc-4364-adaa-cd79c3b2c774",
   "metadata": {},
   "source": [
    "## 4 Pseudocode\n",
    "\n",
    "### Core Research Engine Flow\n",
    "```\n",
    "function conduct_research(query, context, session_id):\n",
    "    initialize_research_state(query, context, session_id)\n",
    "    \n",
    "    for each stage in [1..6]:\n",
    "        display_stage_progress(stage.number, stage.name)\n",
    "        \n",
    "        try:\n",
    "            stage_method = get_stage_method(stage.method_name)\n",
    "            stage_result = stage_method(research_state)\n",
    "            \n",
    "            validate_stage_result(stage_result)\n",
    "            research_state.stages.append(stage_result)\n",
    "            session_manager.update_session_stage(session_id, stage_result)\n",
    "            \n",
    "            sleep(rate_limit_delay)\n",
    "            \n",
    "        catch exception:\n",
    "            log_error(stage.number, exception)\n",
    "            add_fallback_result(research_state, stage, exception)\n",
    "    \n",
    "    confidence_score = calculate_confidence_score(research_state)\n",
    "    finalize_results(research_state, confidence_score)\n",
    "    return final_results\n",
    "```\n",
    "\n",
    "### Dynamic Personalization Engine\n",
    "```\n",
    "function gather_personalization(query, temp_session_id):\n",
    "    user_signals = mode_intelligence.analyze_user_signals(query)\n",
    "    mode_recommendation = mode_intelligence.recommend_conversation_mode(user_signals)\n",
    "    \n",
    "    conversation_state = initialize_conversation(query, temp_session_id)\n",
    "    current_mode = mode_recommendation.recommended_mode\n",
    "    max_questions = get_mode_config(current_mode).max_questions\n",
    "    \n",
    "    question_count = 0\n",
    "    user_responses = []\n",
    "    \n",
    "    while question_count < max_questions:\n",
    "        question = generate_next_question(conversation_state)\n",
    "        if not question: break\n",
    "        \n",
    "        response = ask_user(question)\n",
    "        if not response: continue\n",
    "        \n",
    "        process_user_response(conversation_state, question, response)\n",
    "        user_responses.append(response)\n",
    "        \n",
    "        if question_count >= 2:\n",
    "            engagement_metrics = monitor_engagement(user_responses)\n",
    "            should_switch = should_switch_mode(current_mode, engagement_metrics)\n",
    "            \n",
    "            if should_switch:\n",
    "                new_mode = determine_new_mode(current_mode, engagement_metrics)\n",
    "                transition_between_modes(current_mode, new_mode)\n",
    "                current_mode = new_mode\n",
    "        \n",
    "        question_count++\n",
    "    \n",
    "    return convert_conversation_to_context(conversation_state)\n",
    "```\n",
    "\n",
    "### Session Management\n",
    "```\n",
    "function create_session(query, context):\n",
    "    validate_query(query)\n",
    "    validate_context_data(context)\n",
    "    \n",
    "    session_id = generate_session_id()  // DRA_YYYYMMDD_HHMMSS_μsec\n",
    "    \n",
    "    session_data = {\n",
    "        session_id: session_id,\n",
    "        created_at: current_timestamp(),\n",
    "        query: query,\n",
    "        context: context,\n",
    "        research_results: empty_results(),\n",
    "        status: \"created\"\n",
    "    }\n",
    "    \n",
    "    save_session_to_file(session_data)\n",
    "    return session_data\n",
    "```\n",
    "\n",
    "### Input Validation & Sanitization\n",
    "```\n",
    "function validate_query(query):\n",
    "    if not is_string(query) or is_empty(query):\n",
    "        throw ValidationError(\"Query must be non-empty string\")\n",
    "    \n",
    "    sanitized = sanitize_string(query, max_length=500)\n",
    "    \n",
    "    if length(sanitized) < 5:\n",
    "        throw ValidationError(\"Query too short\")\n",
    "    \n",
    "    if not contains_alphabetic_chars(sanitized):\n",
    "        throw ValidationError(\"Query must contain letters\")\n",
    "    \n",
    "    return sanitized\n",
    "\n",
    "function sanitize_string(text, max_length):\n",
    "    text = remove_control_characters(text)\n",
    "    text = remove_sql_injection_patterns(text)\n",
    "    text = remove_xss_patterns(text)\n",
    "    text = remove_command_injection_patterns(text)\n",
    "    text = remove_dangerous_characters(text)\n",
    "    return truncate(strip(text), max_length)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f8e01d-65e6-4f93-a893-f9bb183813ea",
   "metadata": {},
   "source": [
    "## 5 Code-Level Walk-through\n",
    "\n",
    "**AI Integration Pattern** (`core/research_engine.py:99-150`):\n",
    "- Uses Google Gemini 2.5 with safety settings and retry logic\n",
    "- Exponential backoff for rate limiting with configurable delays\n",
    "- Structured JSON response parsing with fallback error handling\n",
    "- Each stage has dedicated prompt templates optimized for research quality\n",
    "\n",
    "**Dynamic Conversation Intelligence** (`core/conversation_mode_intelligence.py:90-200`):\n",
    "- Four adaptive modes: Quick (3q), Standard (6q), Deep (12q), Adaptive (8q)\n",
    "- Real-time engagement monitoring with metrics-based mode switching\n",
    "- AI-powered urgency detection and complexity assessment\n",
    "- Natural conversation flow with context-aware question generation\n",
    "\n",
    "**Security Implementation** (`utils/validators.py:45-120`):\n",
    "- Comprehensive input sanitization against SQL injection, XSS, command injection\n",
    "- Path traversal prevention with project root validation\n",
    "- Control character filtering and dangerous pattern detection\n",
    "- Configurable limits with settings-driven validation rules\n",
    "\n",
    "**Session Persistence** (`utils/session_manager.py:40-90`):\n",
    "- Atomic JSON operations with secure file permissions (600)\n",
    "- Graceful degradation for corrupted sessions\n",
    "- Automatic cleanup of stale/incomplete sessions\n",
    "- Microsecond-precision session ID generation\n",
    "\n",
    "**Progressive Research Stages** (`core/research_engine.py:180-350`):\n",
    "- Stage 1: Broad information gathering with source reliability scoring\n",
    "- Stage 2: Fact-checking and validation with confidence assessment\n",
    "- Stage 3: Gap-filling research with targeted follow-up queries\n",
    "- Stage 4: Systematic comparative analysis with scoring matrices\n",
    "- Stage 5: Synthesis integration with pattern identification\n",
    "- Stage 6: Final conclusions with personalized recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad5104-31dd-41f4-9e4f-c7df98e1d9a2",
   "metadata": {},
   "source": [
    "## 6 Improvement Radar\n",
    "\n",
    "### Maintainability\n",
    "**• Async/Await Integration** - Current sync calls block during AI requests\n",
    "*Why*: 6-stage research process can take 60+ seconds; blocking UX is poor\n",
    "*Fix*: Convert Gemini calls to async, add progress indicators, enable cancellation\n",
    "\n",
    "**• Configuration Hot Reloading** - Settings require restart to take effect\n",
    "*Why*: Development workflow slowed by constant restarts for config changes  \n",
    "*Fix*: Add file watcher for settings.yaml, implement reload without restart\n",
    "\n",
    "**• Error Recovery Strategies** - Limited graceful degradation patterns\n",
    "*Why*: AI failures cascade to complete research failure vs partial results\n",
    "*Fix*: Implement research stage checkpointing, resume capability, offline mode\n",
    "\n",
    "### Performance\n",
    "**• Response Caching** - No caching of similar queries or AI responses\n",
    "*Why*: Repeated similar queries waste API calls and user time\n",
    "*Fix*: Implement semantic similarity caching with TTL, query fingerprinting\n",
    "\n",
    "**• Parallel Stage Execution** - Sequential 6-stage process is inherently slow\n",
    "*Why*: Some stages could run concurrently (validation + clarification)\n",
    "*Fix*: Dependency analysis, async stage orchestration, result merging\n",
    "\n",
    "**• Memory Optimization** - Full conversation history kept in memory\n",
    "*Why*: Long conversations consume excessive RAM, no cleanup\n",
    "*Fix*: Implement conversation state compression, LRU eviction, streaming\n",
    "\n",
    "### Security  \n",
    "**• API Key Exposure** - Keys logged in debug mode, stored in plaintext\n",
    "*Why*: Sensitive credentials visible in logs and config files\n",
    "*Fix*: Credential masking in logs, encrypted storage, key rotation\n",
    "\n",
    "**• Injection Vulnerabilities** - Incomplete sanitization in AI prompts\n",
    "*Why*: User input directly interpolated into AI prompts without escaping\n",
    "*Fix*: Parameterized prompt templates, output validation, context sandboxing\n",
    "\n",
    "**• Resource Exhaustion** - No rate limiting on user requests\n",
    "*Why*: Malicious users could exhaust API quotas or overwhelm system\n",
    "*Fix*: User-based rate limiting, request queuing, resource monitoring\n",
    "\n",
    "### API/UX\n",
    "**• Conversation Recovery** - No ability to resume interrupted conversations\n",
    "*Why*: Users lose progress if session terminates during personalization\n",
    "*Fix*: Conversation state persistence, resume prompts, partial result saving\n",
    "\n",
    "**• Real-time Feedback** - Limited progress indication during long operations\n",
    "*Why*: Users unsure if system is working during 60+ second research cycles\n",
    "*Fix*: WebSocket progress events, estimated completion times, cancellation\n",
    "\n",
    "**• Export Capabilities** - Reports only available as markdown files\n",
    "*Why*: Users need PDF, DOCX, or structured data formats for sharing\n",
    "*Fix*: Multi-format export pipeline, template system, print-friendly CSS\n",
    "\n",
    "### LLM-friendliness\n",
    "**• Code Documentation** - Inconsistent docstring patterns across modules\n",
    "*Why*: AI code assistants struggle with inconsistent documentation styles\n",
    "*Fix*: Standardize to Google/NumPy docstring format, add type hints\n",
    "\n",
    "**• Prompt Engineering** - Hard-coded prompts scattered throughout codebase\n",
    "*Why*: Difficult for AI to suggest prompt improvements or variations\n",
    "*Fix*: Centralized prompt template system, version control, A/B testing\n",
    "\n",
    "**• Function Granularity** - Some methods exceed 100 lines, complex logic\n",
    "*Why*: AI cannot easily understand or suggest improvements to large functions\n",
    "*Fix*: Single responsibility refactoring, pure function extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a7621-5abc-4f03-9136-b198d1457c6a",
   "metadata": {},
   "source": [
    "## 7 Ready-Made Follow-Up Prompts\n",
    "\n",
    "| **Category** | **Prompt** | **Expected Output** |\n",
    "|--------------|------------|-------------------|\n",
    "| **Feature Development** | \"Add a new conversation mode called 'Expert' that skips basic questions and focuses on advanced criteria for users with domain expertise.\" | Implementation in conversation_mode_intelligence.py with new mode config |\n",
    "| **Performance Optimization** | \"Implement response caching for similar queries using semantic similarity matching to reduce API calls by 40%.\" | Caching layer in research_engine.py with similarity threshold settings |\n",
    "| **Security Enhancement** | \"Add input validation for AI prompt injection attacks where users try to manipulate the research direction through crafted queries.\" | Enhanced sanitization in validators.py with prompt injection detection |\n",
    "| **Export Feature** | \"Create a PDF export option for reports with professional formatting, charts, and company branding support.\" | New export module with PDF generation pipeline |\n",
    "| **API Integration** | \"Add support for OpenAI GPT models as an alternative to Gemini with automatic fallback capability.\" | AI abstraction layer with multi-provider support |\n",
    "| **Analytics** | \"Implement usage analytics to track conversation effectiveness, user satisfaction, and research quality metrics.\" | Analytics module with privacy-compliant data collection |\n",
    "| **Testing Enhancement** | \"Create integration tests that use a mock AI service to test the complete research flow without API costs.\" | Mock service implementation and comprehensive flow tests |\n",
    "| **Configuration** | \"Add a web-based configuration UI for non-technical users to adjust research parameters and conversation settings.\" | Web interface for settings management |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbdd393-a5ae-443a-a245-4ad749f22561",
   "metadata": {},
   "source": [
    "## 8 Appendix\n",
    "\n",
    "### Glossary of Domain Terms\n",
    "\n",
    "**• Conversation State** - Persistent object tracking user profile, question history, and AI-discovered insights throughout personalization\n",
    "**• Research Stage** - One of six sequential analysis phases: gathering, validation, clarification, comparison, synthesis, conclusions  \n",
    "**• Dynamic Personalization** - AI-driven adaptive questioning that discovers user priorities without predefined categories\n",
    "**• Conversation Mode** - Interaction style (Quick/Standard/Deep/Adaptive) that adjusts question depth and pace\n",
    "**• Session Manager** - Component handling JSON persistence of complete research sessions with unique identifiers\n",
    "**• Confidence Scoring** - 0.0-1.0 metric indicating research quality based on evidence strength and source reliability\n",
    "\n",
    "### External Resources Worth Reading\n",
    "- [Google Gemini API Documentation](https://ai.google.dev/docs) - Integration patterns and safety settings\n",
    "- [Pytest Best Practices](https://docs.pytest.org/en/stable/) - Test organization with custom markers\n",
    "- [Python Security Guide](https://python-security.readthedocs.io/) - Input validation and sanitization patterns\n",
    "- [Conversational AI Design](https://conversational-ai.io/) - User experience patterns for AI dialogue systems\n",
    "\n",
    "### Open Questions & Ambiguous Parts\n",
    "\n",
    "**TODO: Research Engine Stage Dependencies** - Unclear if stages 2-3 could run in parallel, dependency analysis needed\n",
    "\n",
    "**TODO: Conversation Mode Thresholds** - Magic numbers for engagement metrics (response_length > 20 = \"high\") need validation\n",
    "\n",
    "**TODO: API Rate Limiting** - No documentation on Gemini rate limits or quota management strategy  \n",
    "\n",
    "**TODO: Error Recovery** - Partial research results handling when stages 4-6 fail is inconsistent\n",
    "\n",
    "**TODO: Session Cleanup Logic** - 24-hour threshold for \"stale\" sessions seems arbitrary, needs user research\n",
    "\n",
    "**TODO: Memory Management** - No analysis of memory usage patterns for long conversations or large result sets\n",
    "\n",
    "---\n",
    "*Analysis completed by ProjectSage - designed to provide technical teams with actionable insights for codebase evolution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b6476-38fc-4afe-839e-3c7db2525ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
